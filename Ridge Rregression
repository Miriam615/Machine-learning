
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge

#load data
data = pd.read_csv("Advertising.csv")
print(data.head())


#Before using Ridge to regularize, create a linear model first
X = data.drop(["Sales"], axis=1)
y = data["Sales"]

#split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

linreg = LinearRegression()
MSE = cross_val_score(linreg, X, y, scoring="neg_mean_squared_error", cv=5)

mean_MSE = np.mean(MSE)
print(mean_MSE)

# Ridge Regression
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
ridge = Ridge()

alpha_list = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]
parameters = {"alpha":alpha_list}
ridge_regression = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)
ridge_regression.fit(X, y)

best_alpha = ridge_regression.best_params_["alpha"]
best_score = ridge_regression.best_score_
print("Best alpha:", best_alpha)   #if close to 0, then collinearity problem not serious, OLS good enough, if not, Ridge helps a lot
print("Best CV MSE:", best_score)

# 4. Plot: alpha vs CV MSE
cv_mse = ridge_regression.cv_results_["mean_test_score"]

plt.figure(figsize=(8,5))
plt.plot(alpha_list, cv_mse, marker='o')
plt.xscale("log")
plt.xlabel("alpha (log scale)")
plt.ylabel("CV MSE (neg)")
plt.title("Ridge Regression: alpha vs CV MSE")
plt.grid(True)
plt.show()

# 5. Fit final Ridge model and plot coefficients
ridge_best = Ridge(alpha=best_alpha)
ridge_best.fit(X, y)

coef = ridge_best.coef_

plt.figure(figsize=(8,5))
plt.bar(X.columns, coef)
plt.xlabel("Features")
plt.ylabel("Coefficient")
plt.title("Ridge Regression Coefficients (Features to focus on)")
plt.grid(True)
plt.show()
